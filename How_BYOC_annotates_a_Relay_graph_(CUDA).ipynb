{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aquapapaya/BYOC/blob/main/How_BYOC_annotates_a_Relay_graph_(CUDA).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pcRLKpMLu2I"
      },
      "source": [
        "# BYOC Demo\n",
        "**Author**: [Kuen-Wey Lin](https://github.com/aquapapaya)\n",
        "\n",
        "We use a simple Relay graph to walkthrough the BYOC workflow.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "rXbWiPPULu2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "705738b0-5fa4-4abc-d52a-2139de8ec133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://tlcpack.ai/wheels\n",
            "Collecting apache-tvm-cu102\n",
            "  Downloading https://github.com/tlc-pack/tlcpack/releases/download/v0.7.dev1/apache_tvm_cu102-0.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (403.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m403.1/403.1 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (23.2.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (2.2.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (4.4.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (1.25.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (5.9.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (1.11.4)\n",
            "Collecting synr==0.6.0 (from apache-tvm-cu102)\n",
            "  Downloading synr-0.6.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: tornado in /usr/local/lib/python3.10/dist-packages (from apache-tvm-cu102) (6.3.3)\n",
            "Installing collected packages: synr, apache-tvm-cu102\n",
            "Successfully installed apache-tvm-cu102-0.9.0 synr-0.6.0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "%%shell\n",
        "# Installs pre-built binaries including CUDA from https://tlcpack.ai/\n",
        "pip install apache-tvm-cu102 -f https://tlcpack.ai/wheels"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tvm\n",
        "from tvm import relay\n",
        "import tvm.relay.testing"
      ],
      "metadata": {
        "id": "oDc5WQtXSsxi"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since the entire Relay graph is pretty large, here we use a simple Relay pass to show the total number of operators it has and what they are."
      ],
      "metadata": {
        "id": "v3-atrc2QyBI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def profile_graph(func):\n",
        "    class OpProfiler(tvm.relay.ExprVisitor):\n",
        "        def __init__(self):\n",
        "            super().__init__()\n",
        "            self.ops = {}\n",
        "\n",
        "        def visit_call(self, call):\n",
        "            op = call.op\n",
        "            if op not in self.ops:\n",
        "                self.ops[op] = 0\n",
        "            self.ops[op] += 1\n",
        "            super().visit_call(call)\n",
        "\n",
        "        def get_cuda_graph_num(self):\n",
        "            cnt = 0\n",
        "            for op in self.ops:\n",
        "                if str(op).find(\"cuda\") != -1:\n",
        "                    cnt += 1\n",
        "            return cnt\n",
        "\n",
        "    profiler = OpProfiler()\n",
        "    profiler.visit(func)\n",
        "    print(\"Total number of operators: %d\" % sum(profiler.ops.values()))\n",
        "    print(\"Detail breakdown\")\n",
        "    for op, count in profiler.ops.items():\n",
        "        print(\"\\t%s: %d\" % (op, count))\n",
        "    print(\"cuda subgraph #: %d\" % profiler.get_cuda_graph_num())"
      ],
      "metadata": {
        "id": "qFV9NY9xLhzN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_9WsjB2Lu2L"
      },
      "source": [
        "Here we demonstrate how BYOC annotates a Relay graph.\n",
        "Let's first define a simple Relay graph with supported and unsupported operators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Sw5iQZPHLu2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30c0658d-0deb-4ff2-8905-46d5a0ce1002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %0 = nn.conv2d(%data, %conv1_1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %1 = nn.bias_add(%0, %conv1_1_bias) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %2 = nn.relu(%1) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %3 = nn.max_pool2d(%2, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %4 = nn.conv2d(%3, %conv2_1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %5 = nn.bias_add(%4, %conv2_1_bias) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %6 = nn.relu(%5) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %7 = nn.max_pool2d(%6, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %8 = nn.conv2d(%7, %conv3_1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %9 = nn.bias_add(%8, %conv3_1_bias) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %10 = nn.relu(%9) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %11 = nn.conv2d(%10, %conv3_2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %12 = nn.bias_add(%11, %conv3_2_bias) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %13 = nn.relu(%12) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %14 = nn.max_pool2d(%13, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %15 = nn.conv2d(%14, %conv4_1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %16 = nn.bias_add(%15, %conv4_1_bias) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %17 = nn.relu(%16) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %18 = nn.conv2d(%17, %conv4_2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %19 = nn.bias_add(%18, %conv4_2_bias) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %20 = nn.relu(%19) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %21 = nn.max_pool2d(%20, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %22 = nn.conv2d(%21, %conv5_1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %23 = nn.bias_add(%22, %conv5_1_bias) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %24 = nn.relu(%23) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %25 = nn.conv2d(%24, %conv5_2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %26 = nn.bias_add(%25, %conv5_2_bias) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %27 = nn.relu(%26) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %28 = nn.max_pool2d(%27, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %29 = nn.batch_flatten(%28) /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %30 = nn.dense(%29, %fc6_weight, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %31 = nn.bias_add(%30, %fc6_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %32 = nn.relu(%31) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %33 = nn.dropout(%32) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %34 = %33.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %35 = nn.dense(%34, %fc7_weight, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %36 = nn.bias_add(%35, %fc7_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %37 = nn.relu(%36) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %38 = nn.dropout(%37) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %39 = %38.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %40 = nn.dense(%39, %fc8_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %41 = nn.bias_add(%40, %fc8_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  nn.softmax(%41) /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n",
            "Total number of operators: 41\n",
            "Detail breakdown\n",
            "\tnn.softmax: 1\n",
            "\tnn.bias_add: 11\n",
            "\tnn.dense: 3\n",
            "\tnn.dropout: 2\n",
            "\tnn.relu: 10\n",
            "\tnn.batch_flatten: 1\n",
            "\tnn.max_pool2d: 5\n",
            "\tnn.conv2d: 8\n",
            "cuda subgraph #: 0\n"
          ]
        }
      ],
      "source": [
        "# Define the neural network\n",
        "# Get the symbol definition and random weight of a network\n",
        "mod, params = relay.testing.vgg.get_workload(batch_size=1, num_classes=1000,\n",
        "    image_shape=(3, 224, 224), dtype='float32', num_layers=11\n",
        ")\n",
        "print(mod)\n",
        "profile_graph(mod[\"main\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fh_aoRJuLu2L"
      },
      "source": [
        "Then we define the annotation rules.\n",
        "Developers can specify both operator-based and pattern-based annotation rules. Here, we define the single operators `dense` is supported. In addition, we also define two supported patterns `(Conv2D - (Bias) - ReLU)`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "SisEgToZLu2L"
      },
      "outputs": [],
      "source": [
        "# Operator-based annotation rules\n",
        "@tvm.ir.register_op_attr(\"nn.dense\", \"target.cuda\")\n",
        "def dense(expr):\n",
        "    return True\n",
        "\n",
        "# Pattern-based annotation rules\n",
        "def make_pattern(with_bias=True):\n",
        "    from tvm.relay.dataflow_pattern import is_op, wildcard\n",
        "    data = wildcard()\n",
        "    weight = wildcard()\n",
        "    bias = wildcard()\n",
        "    conv = is_op(\"nn.conv2d\")(data, weight)\n",
        "    if with_bias:\n",
        "        conv_out = is_op(\"nn.bias_add\")(conv, bias)\n",
        "    else:\n",
        "        conv_out = conv\n",
        "    return is_op(\"nn.relu\")(conv_out)\n",
        "\n",
        "conv2d_bias_relu_pat = (\"cuda.conv2d_relu_with_bias\", make_pattern(with_bias=True))\n",
        "conv2d_relu_pat = (\"cuda.conv2d_relu_wo_bias\", make_pattern(with_bias=False))\n",
        "patterns = [conv2d_bias_relu_pat, conv2d_relu_pat]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S93m1zLpLu2M"
      },
      "source": [
        "Now let's perform pattern-based annotation:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod2 = relay.transform.MergeComposite(patterns)(mod)\n",
        "print(mod2)\n",
        "profile_graph(mod2[\"main\"])"
      ],
      "metadata": {
        "id": "K6vWlSY0ilrf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c15aa56-908c-424b-9fbe-551c4d88d575"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %16 = fn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "    %14 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    %15 = nn.bias_add(%14, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    nn.relu(%15) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %17 = %16(%data, %conv1_1_weight, %conv1_1_bias) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %18 = nn.max_pool2d(%17, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %19 = fn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "    %12 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    %13 = nn.bias_add(%12, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    nn.relu(%13) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %20 = %19(%18, %conv2_1_weight, %conv2_1_bias) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %21 = nn.max_pool2d(%20, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %22 = fn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %10 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %11 = nn.bias_add(%10, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%11) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %23 = %22(%21, %conv3_1_weight, %conv3_1_bias) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %24 = fn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %8 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %9 = nn.bias_add(%8, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%9) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %25 = %24(%23, %conv3_2_weight, %conv3_2_bias) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %26 = nn.max_pool2d(%25, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %27 = fn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %6 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %7 = nn.bias_add(%6, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%7) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %28 = %27(%26, %conv4_1_weight, %conv4_1_bias) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %29 = fn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %4 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %5 = nn.bias_add(%4, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%5) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %30 = %29(%28, %conv4_2_weight, %conv4_2_bias) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %31 = nn.max_pool2d(%30, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %32 = fn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %2 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %3 = nn.bias_add(%2, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%3) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %33 = %32(%31, %conv5_1_weight, %conv5_1_bias) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %34 = fn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %1 = nn.bias_add(%0, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %35 = %34(%33, %conv5_2_weight, %conv5_2_bias) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %36 = nn.max_pool2d(%35, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %37 = nn.batch_flatten(%36) /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %38 = nn.dense(%37, %fc6_weight, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %39 = nn.bias_add(%38, %fc6_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %40 = nn.relu(%39) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %41 = nn.dropout(%40) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %42 = %41.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %43 = nn.dense(%42, %fc7_weight, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %44 = nn.bias_add(%43, %fc7_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %45 = nn.relu(%44) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %46 = nn.dropout(%45) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %47 = %46.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %48 = nn.dense(%47, %fc8_weight, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %49 = nn.bias_add(%48, %fc8_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  nn.softmax(%49) /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n",
            "Total number of operators: 49\n",
            "Detail breakdown\n",
            "\tnn.softmax: 1\n",
            "\tnn.bias_add: 11\n",
            "\tnn.dense: 3\n",
            "\tnn.dropout: 2\n",
            "\tnn.relu: 10\n",
            "\tnn.batch_flatten: 1\n",
            "\tnn.max_pool2d: 5\n",
            "\tfn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */: 1\n",
            "\tnn.conv2d: 8\n",
            "\tfn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */: 1\n",
            "\tfn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */: 1\n",
            "\tfn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "} /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */: 1\n",
            "\tfn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "} /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */: 1\n",
            "\tfn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "} /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */: 1\n",
            "\tfn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "} /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */: 1\n",
            "\tfn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "} /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */: 1\n",
            "cuda subgraph #: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A composite function has two specialized attributes -- `PartitionedFromPattern` and `Composite`:\n",
        "*   PartitionedFromPattern: Indicate the operators in the function body.\n",
        "*   Composite: Indicate the pattern name we defined.\n",
        "\n",
        "Next, let's continue to apply the operator-based annotation rules:"
      ],
      "metadata": {
        "id": "tQXH6ryAi8NB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod3 = relay.transform.AnnotateTarget(\"cuda\")(mod2)\n",
        "print(mod3)\n",
        "profile_graph(mod3[\"main\"])"
      ],
      "metadata": {
        "id": "8YaCKNNrj4xr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8447c40-18f3-4da6-fa44-a1f910ca174b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %16 = annotation.compiler_begin(%data, compiler=\"cuda\") /* ty=Tensor[(1, 3, 224, 224), float32] */;\n",
            "  %17 = annotation.compiler_begin(%conv1_1_weight, compiler=\"cuda\") /* ty=Tensor[(64, 3, 3, 3), float32] */;\n",
            "  %18 = annotation.compiler_begin(%conv1_1_bias, compiler=\"cuda\") /* ty=Tensor[(64), float32] */;\n",
            "  %19 = fn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "    %14 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    %15 = nn.bias_add(%14, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    nn.relu(%15) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %20 = %19(%16, %17, %18) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %21 = annotation.compiler_end(%20, compiler=\"cuda\") /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %22 = annotation.compiler_begin(%21, compiler=\"default\") /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %23 = nn.max_pool2d(%22, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %24 = annotation.compiler_end(%23, compiler=\"default\") /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %25 = annotation.compiler_begin(%24, compiler=\"cuda\") /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %26 = annotation.compiler_begin(%conv2_1_weight, compiler=\"cuda\") /* ty=Tensor[(128, 64, 3, 3), float32] */;\n",
            "  %27 = annotation.compiler_begin(%conv2_1_bias, compiler=\"cuda\") /* ty=Tensor[(128), float32] */;\n",
            "  %28 = fn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "    %12 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    %13 = nn.bias_add(%12, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    nn.relu(%13) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %29 = %28(%25, %26, %27) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %30 = annotation.compiler_end(%29, compiler=\"cuda\") /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %31 = annotation.compiler_begin(%30, compiler=\"default\") /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %32 = nn.max_pool2d(%31, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %33 = annotation.compiler_end(%32, compiler=\"default\") /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %34 = annotation.compiler_begin(%33, compiler=\"cuda\") /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %35 = annotation.compiler_begin(%conv3_1_weight, compiler=\"cuda\") /* ty=Tensor[(256, 128, 3, 3), float32] */;\n",
            "  %36 = annotation.compiler_begin(%conv3_1_bias, compiler=\"cuda\") /* ty=Tensor[(256), float32] */;\n",
            "  %37 = fn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %10 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %11 = nn.bias_add(%10, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%11) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %38 = %37(%34, %35, %36) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %39 = annotation.compiler_end(%38, compiler=\"cuda\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %40 = annotation.compiler_begin(%39, compiler=\"cuda\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %41 = annotation.compiler_begin(%conv3_2_weight, compiler=\"cuda\") /* ty=Tensor[(256, 256, 3, 3), float32] */;\n",
            "  %42 = annotation.compiler_begin(%conv3_2_bias, compiler=\"cuda\") /* ty=Tensor[(256), float32] */;\n",
            "  %43 = fn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %8 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %9 = nn.bias_add(%8, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%9) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %44 = %43(%40, %41, %42) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %45 = annotation.compiler_end(%44, compiler=\"cuda\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %46 = annotation.compiler_begin(%45, compiler=\"default\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %47 = nn.max_pool2d(%46, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %48 = annotation.compiler_end(%47, compiler=\"default\") /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %49 = annotation.compiler_begin(%48, compiler=\"cuda\") /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %50 = annotation.compiler_begin(%conv4_1_weight, compiler=\"cuda\") /* ty=Tensor[(512, 256, 3, 3), float32] */;\n",
            "  %51 = annotation.compiler_begin(%conv4_1_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %52 = fn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %6 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %7 = nn.bias_add(%6, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%7) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %53 = %52(%49, %50, %51) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %54 = annotation.compiler_end(%53, compiler=\"cuda\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %55 = annotation.compiler_begin(%54, compiler=\"cuda\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %56 = annotation.compiler_begin(%conv4_2_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %57 = annotation.compiler_begin(%conv4_2_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %58 = fn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %4 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %5 = nn.bias_add(%4, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%5) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %59 = %58(%55, %56, %57) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %60 = annotation.compiler_end(%59, compiler=\"cuda\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %61 = annotation.compiler_begin(%60, compiler=\"default\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %62 = nn.max_pool2d(%61, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %63 = annotation.compiler_end(%62, compiler=\"default\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %64 = annotation.compiler_begin(%63, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %65 = annotation.compiler_begin(%conv5_1_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %66 = annotation.compiler_begin(%conv5_1_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %67 = fn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %2 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %3 = nn.bias_add(%2, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%3) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %68 = %67(%64, %65, %66) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %69 = annotation.compiler_end(%68, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %70 = annotation.compiler_begin(%69, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %71 = annotation.compiler_begin(%conv5_2_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %72 = annotation.compiler_begin(%conv5_2_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %73 = fn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %1 = nn.bias_add(%0, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %74 = %73(%70, %71, %72) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %75 = annotation.compiler_end(%74, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %76 = annotation.compiler_begin(%75, compiler=\"default\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %77 = nn.max_pool2d(%76, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %78 = annotation.compiler_end(%77, compiler=\"default\") /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %79 = annotation.compiler_begin(%78, compiler=\"default\") /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %80 = nn.batch_flatten(%79) /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %81 = annotation.compiler_end(%80, compiler=\"default\") /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %82 = annotation.compiler_begin(%81, compiler=\"cuda\") /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %83 = annotation.compiler_begin(%fc6_weight, compiler=\"cuda\") /* ty=Tensor[(4096, 25088), float32] */;\n",
            "  %84 = nn.dense(%82, %83, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %85 = annotation.compiler_end(%84, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %86 = annotation.compiler_begin(%85, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %87 = annotation.compiler_begin(%fc6_bias, compiler=\"default\") /* ty=Tensor[(4096), float32] */;\n",
            "  %88 = nn.bias_add(%86, %87, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %89 = annotation.compiler_end(%88, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %90 = annotation.compiler_begin(%89, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %91 = nn.relu(%90) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %92 = annotation.compiler_end(%91, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %93 = annotation.compiler_begin(%92, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %94 = nn.dropout(%93) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %95 = annotation.compiler_end(%94, compiler=\"default\") /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %96 = annotation.compiler_begin(%95, compiler=\"default\") /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %97 = %96.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %98 = annotation.compiler_end(%97, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %99 = annotation.compiler_begin(%98, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %100 = annotation.compiler_begin(%fc7_weight, compiler=\"cuda\") /* ty=Tensor[(4096, 4096), float32] */;\n",
            "  %101 = nn.dense(%99, %100, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %102 = annotation.compiler_end(%101, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %103 = annotation.compiler_begin(%102, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %104 = annotation.compiler_begin(%fc7_bias, compiler=\"default\") /* ty=Tensor[(4096), float32] */;\n",
            "  %105 = nn.bias_add(%103, %104, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %106 = annotation.compiler_end(%105, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %107 = annotation.compiler_begin(%106, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %108 = nn.relu(%107) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %109 = annotation.compiler_end(%108, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %110 = annotation.compiler_begin(%109, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %111 = nn.dropout(%110) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %112 = annotation.compiler_end(%111, compiler=\"default\") /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %113 = annotation.compiler_begin(%112, compiler=\"default\") /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %114 = %113.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %115 = annotation.compiler_end(%114, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %116 = annotation.compiler_begin(%115, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %117 = annotation.compiler_begin(%fc8_weight, compiler=\"cuda\") /* ty=Tensor[(1000, 4096), float32] */;\n",
            "  %118 = nn.dense(%116, %117, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %119 = annotation.compiler_end(%118, compiler=\"cuda\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %120 = annotation.compiler_begin(%119, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %121 = annotation.compiler_begin(%fc8_bias, compiler=\"default\") /* ty=Tensor[(1000), float32] */;\n",
            "  %122 = nn.bias_add(%120, %121, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %123 = annotation.compiler_end(%122, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %124 = annotation.compiler_begin(%123, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %125 = nn.softmax(%124) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  annotation.compiler_end(%125, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n",
            "Total number of operators: 125\n",
            "Detail breakdown\n",
            "\tannotation.compiler_end: 27\n",
            "\tnn.softmax: 1\n",
            "\tannotation.compiler_begin: 49\n",
            "\tnn.bias_add: 11\n",
            "\tnn.dense: 3\n",
            "\tnn.dropout: 2\n",
            "\tnn.relu: 10\n",
            "\tnn.batch_flatten: 1\n",
            "\tnn.max_pool2d: 5\n",
            "\tfn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */: 1\n",
            "\tnn.conv2d: 8\n",
            "\tfn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */: 1\n",
            "\tfn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "} /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */: 1\n",
            "\tfn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "} /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */: 1\n",
            "\tfn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "} /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */: 1\n",
            "\tfn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "} /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */: 1\n",
            "\tfn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "} /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */: 1\n",
            "\tfn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "  %0 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %1 = nn.bias_add(%0, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  nn.relu(%1) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "} /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */: 1\n",
            "cuda subgraph #: 8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mod4 = relay.transform.MergeCompilerRegions()(mod3)\n",
        "print(mod4)"
      ],
      "metadata": {
        "id": "n9C2bBQOlRZE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95010a62-a91d-45f3-dee6-0d862583b4e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %16 = annotation.compiler_begin(%data, compiler=\"cuda\") /* ty=Tensor[(1, 3, 224, 224), float32] */;\n",
            "  %17 = annotation.compiler_begin(%conv1_1_weight, compiler=\"cuda\") /* ty=Tensor[(64, 3, 3, 3), float32] */;\n",
            "  %18 = annotation.compiler_begin(%conv1_1_bias, compiler=\"cuda\") /* ty=Tensor[(64), float32] */;\n",
            "  %19 = fn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "    %14 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    %15 = nn.bias_add(%14, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    nn.relu(%15) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %20 = %19(%16, %17, %18) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %21 = annotation.compiler_end(%20, compiler=\"cuda\") /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %22 = annotation.compiler_begin(%21, compiler=\"default\") /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %23 = nn.max_pool2d(%22, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %24 = annotation.compiler_end(%23, compiler=\"default\") /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %25 = annotation.compiler_begin(%24, compiler=\"cuda\") /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %26 = annotation.compiler_begin(%conv2_1_weight, compiler=\"cuda\") /* ty=Tensor[(128, 64, 3, 3), float32] */;\n",
            "  %27 = annotation.compiler_begin(%conv2_1_bias, compiler=\"cuda\") /* ty=Tensor[(128), float32] */;\n",
            "  %28 = fn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "    %12 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    %13 = nn.bias_add(%12, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    nn.relu(%13) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %29 = %28(%25, %26, %27) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %30 = annotation.compiler_end(%29, compiler=\"cuda\") /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %31 = annotation.compiler_begin(%30, compiler=\"default\") /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %32 = nn.max_pool2d(%31, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %33 = annotation.compiler_end(%32, compiler=\"default\") /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %34 = annotation.compiler_begin(%33, compiler=\"cuda\") /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %35 = annotation.compiler_begin(%conv3_1_weight, compiler=\"cuda\") /* ty=Tensor[(256, 128, 3, 3), float32] */;\n",
            "  %36 = annotation.compiler_begin(%conv3_1_bias, compiler=\"cuda\") /* ty=Tensor[(256), float32] */;\n",
            "  %37 = fn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %10 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %11 = nn.bias_add(%10, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%11) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %38 = %37(%34, %35, %36) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %39 = annotation.compiler_begin(%conv3_2_weight, compiler=\"cuda\") /* ty=Tensor[(256, 256, 3, 3), float32] */;\n",
            "  %40 = annotation.compiler_begin(%conv3_2_bias, compiler=\"cuda\") /* ty=Tensor[(256), float32] */;\n",
            "  %41 = fn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %8 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %9 = nn.bias_add(%8, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%9) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %42 = %41(%38, %39, %40) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %43 = annotation.compiler_end(%42, compiler=\"cuda\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %44 = annotation.compiler_begin(%43, compiler=\"default\") /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %45 = nn.max_pool2d(%44, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %46 = annotation.compiler_end(%45, compiler=\"default\") /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %47 = annotation.compiler_begin(%46, compiler=\"cuda\") /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %48 = annotation.compiler_begin(%conv4_1_weight, compiler=\"cuda\") /* ty=Tensor[(512, 256, 3, 3), float32] */;\n",
            "  %49 = annotation.compiler_begin(%conv4_1_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %50 = fn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %6 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %7 = nn.bias_add(%6, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%7) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %51 = %50(%47, %48, %49) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %52 = annotation.compiler_begin(%conv4_2_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %53 = annotation.compiler_begin(%conv4_2_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %54 = fn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %4 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %5 = nn.bias_add(%4, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%5) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %55 = %54(%51, %52, %53) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %56 = annotation.compiler_end(%55, compiler=\"cuda\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %57 = annotation.compiler_begin(%56, compiler=\"default\") /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %58 = nn.max_pool2d(%57, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %59 = annotation.compiler_end(%58, compiler=\"default\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %60 = annotation.compiler_begin(%59, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %61 = annotation.compiler_begin(%conv5_1_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %62 = annotation.compiler_begin(%conv5_1_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %63 = fn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %2 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %3 = nn.bias_add(%2, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%3) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %64 = %63(%60, %61, %62) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %65 = annotation.compiler_begin(%conv5_2_weight, compiler=\"cuda\") /* ty=Tensor[(512, 512, 3, 3), float32] */;\n",
            "  %66 = annotation.compiler_begin(%conv5_2_bias, compiler=\"cuda\") /* ty=Tensor[(512), float32] */;\n",
            "  %67 = fn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %0 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %1 = nn.bias_add(%0, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%1) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %68 = %67(%64, %65, %66) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %69 = annotation.compiler_end(%68, compiler=\"cuda\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %70 = annotation.compiler_begin(%69, compiler=\"default\") /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %71 = nn.max_pool2d(%70, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %72 = nn.batch_flatten(%71) /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %73 = annotation.compiler_end(%72, compiler=\"default\") /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %74 = annotation.compiler_begin(%73, compiler=\"cuda\") /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %75 = annotation.compiler_begin(%fc6_weight, compiler=\"cuda\") /* ty=Tensor[(4096, 25088), float32] */;\n",
            "  %76 = nn.dense(%74, %75, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %77 = annotation.compiler_end(%76, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %78 = annotation.compiler_begin(%77, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %79 = annotation.compiler_begin(%fc6_bias, compiler=\"default\") /* ty=Tensor[(4096), float32] */;\n",
            "  %80 = nn.bias_add(%78, %79, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %81 = nn.relu(%80) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %82 = nn.dropout(%81) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %83 = %82.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %84 = annotation.compiler_end(%83, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %85 = annotation.compiler_begin(%84, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %86 = annotation.compiler_begin(%fc7_weight, compiler=\"cuda\") /* ty=Tensor[(4096, 4096), float32] */;\n",
            "  %87 = nn.dense(%85, %86, units=4096) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %88 = annotation.compiler_end(%87, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %89 = annotation.compiler_begin(%88, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %90 = annotation.compiler_begin(%fc7_bias, compiler=\"default\") /* ty=Tensor[(4096), float32] */;\n",
            "  %91 = nn.bias_add(%89, %90, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %92 = nn.relu(%91) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %93 = nn.dropout(%92) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %94 = %93.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %95 = annotation.compiler_end(%94, compiler=\"default\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %96 = annotation.compiler_begin(%95, compiler=\"cuda\") /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %97 = annotation.compiler_begin(%fc8_weight, compiler=\"cuda\") /* ty=Tensor[(1000, 4096), float32] */;\n",
            "  %98 = nn.dense(%96, %97, units=1000) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %99 = annotation.compiler_end(%98, compiler=\"cuda\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %100 = annotation.compiler_begin(%99, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %101 = annotation.compiler_begin(%fc8_bias, compiler=\"default\") /* ty=Tensor[(1000), float32] */;\n",
            "  %102 = nn.bias_add(%100, %101, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %103 = nn.softmax(%102) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  annotation.compiler_end(%103, compiler=\"default\") /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Almost all nodes in the graph are annotated with `compiler_begin` and `compiler_end` nodes. `compiler_*` nodes has an attribute `compiler` to indicate which target should this node go. In this example, it can be `default` or `cuda`.\n",
        "\n",
        "Composite function calls are also annotated with `compiler=cuda`, indicating that this entire function can be offloaded.\n",
        "\n",
        "We use the pass, `MergeCompilerRegion`, to merge them so that we can minimize the number of subgraphs.\n",
        "\n",
        "Finally, let's partition this graph:"
      ],
      "metadata": {
        "id": "XvjHEZzSBOOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod5 = relay.transform.PartitionGraph()(mod4)\n",
        "print(mod5)"
      ],
      "metadata": {
        "id": "nL1d4RxNDnfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4e909fc-34dd-4507-f658-60b0c99bcf46"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %0 = @tvmgen_default_cuda_main_0(%data, %conv1_1_weight, %conv1_1_bias) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %1 = nn.max_pool2d(%0, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 64, 112, 112), float32] */;\n",
            "  %2 = @tvmgen_default_cuda_main_3(%1, %conv2_1_weight, %conv2_1_bias) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %3 = nn.max_pool2d(%2, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 128, 56, 56), float32] */;\n",
            "  %4 = @tvmgen_default_cuda_main_6(%3, %conv3_1_weight, %conv3_1_bias, %conv3_2_weight, %conv3_2_bias) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %5 = nn.max_pool2d(%4, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 256, 28, 28), float32] */;\n",
            "  %6 = @tvmgen_default_cuda_main_11(%5, %conv4_1_weight, %conv4_1_bias, %conv4_2_weight, %conv4_2_bias) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %7 = nn.max_pool2d(%6, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %8 = @tvmgen_default_cuda_main_16(%7, %conv5_1_weight, %conv5_1_bias, %conv5_2_weight, %conv5_2_bias) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]) /* ty=Tensor[(1, 512, 7, 7), float32] */;\n",
            "  %10 = nn.batch_flatten(%9) /* ty=Tensor[(1, 25088), float32] */;\n",
            "  %11 = @tvmgen_default_cuda_main_21(%10, %fc6_weight) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %12 = nn.bias_add(%11, %fc6_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %13 = nn.relu(%12) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %14 = nn.dropout(%13) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %15 = %14.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %16 = @tvmgen_default_cuda_main_23(%15, %fc7_weight) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %17 = nn.bias_add(%16, %fc7_bias, axis=-1) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %18 = nn.relu(%17) /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %19 = nn.dropout(%18) /* ty=(Tensor[(1, 4096), float32], Tensor[(1, 4096), float32]) */;\n",
            "  %20 = %19.0 /* ty=Tensor[(1, 4096), float32] */;\n",
            "  %21 = @tvmgen_default_cuda_main_25(%20, %fc8_weight) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  %22 = nn.bias_add(%21, %fc8_bias, axis=-1) /* ty=Tensor[(1, 1000), float32] */;\n",
            "  nn.softmax(%22) /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_0(%cuda_0_i0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %cuda_0_i1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %cuda_0_i2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_0\", Primitive=1) -> Tensor[(1, 64, 224, 224), float32] {\n",
            "  %25 = fn (%FunctionVar_7_0: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %FunctionVar_7_1: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %FunctionVar_7_2: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 64, 224, 224), float32] {\n",
            "    %23 = nn.conv2d(%FunctionVar_7_0, %FunctionVar_7_1, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    %24 = nn.bias_add(%23, %FunctionVar_7_2) /* ty=Tensor[(1, 64, 224, 224), float32] */;\n",
            "    nn.relu(%24) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 3, 224, 224), float32], Tensor[(64, 3, 3, 3), float32], Tensor[(64), float32]) -> Tensor[(1, 64, 224, 224), float32] */;\n",
            "  %25(%cuda_0_i0, %cuda_0_i1, %cuda_0_i2) /* ty=Tensor[(1, 64, 224, 224), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_11(%cuda_11_i0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %cuda_11_i1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %cuda_11_i2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %cuda_11_i3: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %cuda_11_i4: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_11\", Primitive=1) -> Tensor[(1, 512, 28, 28), float32] {\n",
            "  %30 = fn (%FunctionVar_3_0: Tensor[(1, 256, 28, 28), float32] /* ty=Tensor[(1, 256, 28, 28), float32] */, %FunctionVar_3_1: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %FunctionVar_3_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %28 = nn.conv2d(%FunctionVar_3_0, %FunctionVar_3_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %29 = nn.bias_add(%28, %FunctionVar_3_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%29) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 28, 28), float32], Tensor[(512, 256, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %31 = %30(%cuda_11_i0, %cuda_11_i1, %cuda_11_i2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %32 = fn (%FunctionVar_2_0: Tensor[(1, 512, 28, 28), float32] /* ty=Tensor[(1, 512, 28, 28), float32] */, %FunctionVar_2_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_2_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 28, 28), float32] {\n",
            "    %26 = nn.conv2d(%FunctionVar_2_0, %FunctionVar_2_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    %27 = nn.bias_add(%26, %FunctionVar_2_2) /* ty=Tensor[(1, 512, 28, 28), float32] */;\n",
            "    nn.relu(%27) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 28, 28), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 28, 28), float32] */;\n",
            "  %32(%31, %cuda_11_i3, %cuda_11_i4) /* ty=Tensor[(1, 512, 28, 28), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_16(%cuda_16_i0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %cuda_16_i1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %cuda_16_i2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %cuda_16_i3: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %cuda_16_i4: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_16\", Primitive=1) -> Tensor[(1, 512, 14, 14), float32] {\n",
            "  %37 = fn (%FunctionVar_1_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_1_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_1_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %35 = nn.conv2d(%FunctionVar_1_0, %FunctionVar_1_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %36 = nn.bias_add(%35, %FunctionVar_1_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%36) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %38 = %37(%cuda_16_i0, %cuda_16_i1, %cuda_16_i2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %39 = fn (%FunctionVar_0_0: Tensor[(1, 512, 14, 14), float32] /* ty=Tensor[(1, 512, 14, 14), float32] */, %FunctionVar_0_1: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %FunctionVar_0_2: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 512, 14, 14), float32] {\n",
            "    %33 = nn.conv2d(%FunctionVar_0_0, %FunctionVar_0_1, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    %34 = nn.bias_add(%33, %FunctionVar_0_2) /* ty=Tensor[(1, 512, 14, 14), float32] */;\n",
            "    nn.relu(%34) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 512, 14, 14), float32], Tensor[(512, 512, 3, 3), float32], Tensor[(512), float32]) -> Tensor[(1, 512, 14, 14), float32] */;\n",
            "  %39(%38, %cuda_16_i3, %cuda_16_i4) /* ty=Tensor[(1, 512, 14, 14), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_21(%cuda_21_i0: Tensor[(1, 25088), float32] /* ty=Tensor[(1, 25088), float32] */, %cuda_21_i1: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_21\", Primitive=1) -> Tensor[(1, 4096), float32] {\n",
            "  nn.dense(%cuda_21_i0, %cuda_21_i1, units=4096) /* ty=Tensor[(1, 4096), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_23(%cuda_23_i0: Tensor[(1, 4096), float32] /* ty=Tensor[(1, 4096), float32] */, %cuda_23_i1: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_23\", Primitive=1) -> Tensor[(1, 4096), float32] {\n",
            "  nn.dense(%cuda_23_i0, %cuda_23_i1, units=4096) /* ty=Tensor[(1, 4096), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_25(%cuda_25_i0: Tensor[(1, 4096), float32] /* ty=Tensor[(1, 4096), float32] */, %cuda_25_i1: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_25\", Primitive=1) -> Tensor[(1, 1000), float32] {\n",
            "  nn.dense(%cuda_25_i0, %cuda_25_i1, units=1000) /* ty=Tensor[(1, 1000), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_3(%cuda_3_i0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %cuda_3_i1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %cuda_3_i2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_3\", Primitive=1) -> Tensor[(1, 128, 112, 112), float32] {\n",
            "  %42 = fn (%FunctionVar_6_0: Tensor[(1, 64, 112, 112), float32] /* ty=Tensor[(1, 64, 112, 112), float32] */, %FunctionVar_6_1: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %FunctionVar_6_2: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 128, 112, 112), float32] {\n",
            "    %40 = nn.conv2d(%FunctionVar_6_0, %FunctionVar_6_1, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    %41 = nn.bias_add(%40, %FunctionVar_6_2) /* ty=Tensor[(1, 128, 112, 112), float32] */;\n",
            "    nn.relu(%41) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 64, 112, 112), float32], Tensor[(128, 64, 3, 3), float32], Tensor[(128), float32]) -> Tensor[(1, 128, 112, 112), float32] */;\n",
            "  %42(%cuda_3_i0, %cuda_3_i1, %cuda_3_i2) /* ty=Tensor[(1, 128, 112, 112), float32] */\n",
            "}\n",
            "\n",
            "def @tvmgen_default_cuda_main_6(%cuda_6_i0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %cuda_6_i1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %cuda_6_i2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %cuda_6_i3: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %cuda_6_i4: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, Inline=1, Compiler=\"cuda\", global_symbol=\"tvmgen_default_cuda_main_6\", Primitive=1) -> Tensor[(1, 256, 56, 56), float32] {\n",
            "  %47 = fn (%FunctionVar_5_0: Tensor[(1, 128, 56, 56), float32] /* ty=Tensor[(1, 128, 56, 56), float32] */, %FunctionVar_5_1: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %FunctionVar_5_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %45 = nn.conv2d(%FunctionVar_5_0, %FunctionVar_5_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %46 = nn.bias_add(%45, %FunctionVar_5_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%46) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 128, 56, 56), float32], Tensor[(256, 128, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %48 = %47(%cuda_6_i0, %cuda_6_i1, %cuda_6_i2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %49 = fn (%FunctionVar_4_0: Tensor[(1, 256, 56, 56), float32] /* ty=Tensor[(1, 256, 56, 56), float32] */, %FunctionVar_4_1: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %FunctionVar_4_2: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, PartitionedFromPattern=\"nn.conv2d_nn.bias_add_nn.relu_\", Composite=\"cuda.conv2d_relu_with_bias\") -> Tensor[(1, 256, 56, 56), float32] {\n",
            "    %43 = nn.conv2d(%FunctionVar_4_0, %FunctionVar_4_1, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    %44 = nn.bias_add(%43, %FunctionVar_4_2) /* ty=Tensor[(1, 256, 56, 56), float32] */;\n",
            "    nn.relu(%44) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "  } /* ty=fn (Tensor[(1, 256, 56, 56), float32], Tensor[(256, 256, 3, 3), float32], Tensor[(256), float32]) -> Tensor[(1, 256, 56, 56), float32] */;\n",
            "  %49(%48, %cuda_6_i3, %cuda_6_i4) /* ty=Tensor[(1, 256, 56, 56), float32] */\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that 8 subgraphs have been partitioned for `cuda`.\n",
        "\n",
        "\n",
        "\n",
        "1.   @tvmgen_default_cuda_target_main_0\n",
        "2.   @tvmgen_default_cuda_target_main_3\n",
        "3.   @tvmgen_default_cuda_target_main_6\n",
        "4.   @tvmgen_default_cuda_target_main_11\n",
        "5.   @tvmgen_default_cuda_target_main_16\n",
        "6.   @tvmgen_default_cuda_target_main_21\n",
        "7.   @tvmgen_default_cuda_target_main_23\n",
        "8.   @tvmgen_default_cuda_target_main_25\n",
        "\n",
        "Each partitioned function will be sent to the `cuda` codegen for code generation.\n",
        "\n",
        "As a result, you can imagine that the customized codegen only needs to consider the subgraphs without worrying about rest parts of the graph.\n",
        "\n"
      ],
      "metadata": {
        "id": "hVPoX64LGmiA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build (optimize and generate code) your Relay IR\n",
        "\n",
        "At first, we build the original Relay IR to generate LLVM code and\n",
        "run the code using CPU."
      ],
      "metadata": {
        "id": "3niYqnJqsa0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the original Raly IR to generate LLVM code\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "  lib = relay.build(mod, target=\"llvm\", params=params)\n",
        "\n",
        "#print(lib.get_lib().get_source()) # host code\n",
        "#print(lib.get_lib().imported_modules[0].get_source()) # device code\n",
        "\n",
        "print(\"Runtime module structure:\")\n",
        "print(\"\\t %s\" % str(lib.get_lib()))\n",
        "for sub_mod in lib.get_lib().imported_modules:\n",
        "  print(\"\\t  |- %s\" % str(sub_mod))\n",
        "\n",
        "# Create the runtime module for the generated LLVM code\n",
        "import tvm.contrib.graph_executor as runtime\n",
        "run_mod = runtime.GraphModule(lib[\"default\"](tvm.cpu(0)))\n",
        "\n",
        "# Run the runtime module 10 times\n",
        "import time\n",
        "import numpy as np\n",
        "times = []\n",
        "for _ in range(10):\n",
        "  start = time.time()\n",
        "  run_mod.run()\n",
        "  times.append(time.time() - start)\n",
        "print(\"Median inference latency %.2f ms\" % (1000 * np.median(times)))"
      ],
      "metadata": {
        "id": "-LyxGVxVWTNT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1478748c-0cb7-46b6-c716-7c9bdf97cc87"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:autotvm:One or more operators have not been tuned. Please tune your model for better performance. Use DEBUG logging level to see more details.\n",
            "/usr/local/lib/python3.10/dist-packages/tvm/driver/build_module.py:267: UserWarning: target_host parameter is going to be deprecated. Please pass in tvm.target.Target(target, host=target_host) instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Runtime module structure:\n",
            "\t Module(llvm, 5c37c18e8108)\n",
            "Median inference latency 1307.70 ms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we dispatch convolution operators to GPU to accelerate this model."
      ],
      "metadata": {
        "id": "uzMs1_dWvX_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# dispatch convolution operators to GPU\n",
        "from tvm.relay.expr_functor import ExprMutator\n",
        "class ScheduleDense(ExprMutator):\n",
        "    def __init__(self, device):\n",
        "        self.device = device\n",
        "        super().__init__()\n",
        "\n",
        "    def visit_call(self, expr):\n",
        "        visit = super().visit_call(expr)\n",
        "        if expr.op == tvm.relay.op.get(\"nn.conv2d\"):\n",
        "            return relay.annotation.on_device(visit, self.device)\n",
        "        else:\n",
        "            return visit\n",
        "func = mod[\"main\"]\n",
        "sched = ScheduleDense(\"cuda\")\n",
        "func = sched.visit(func)\n",
        "mod[\"main\"] = func\n",
        "print('Relay IR:\\n', mod)"
      ],
      "metadata": {
        "id": "ZVyoq46uTS5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9340aa4b-78c0-4145-b703-991d45f845ce"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Relay IR:\n",
            " def @main(%data: Tensor[(1, 3, 224, 224), float32] /* ty=Tensor[(1, 3, 224, 224), float32] */, %conv1_1_weight: Tensor[(64, 3, 3, 3), float32] /* ty=Tensor[(64, 3, 3, 3), float32] */, %conv1_1_bias: Tensor[(64), float32] /* ty=Tensor[(64), float32] */, %conv2_1_weight: Tensor[(128, 64, 3, 3), float32] /* ty=Tensor[(128, 64, 3, 3), float32] */, %conv2_1_bias: Tensor[(128), float32] /* ty=Tensor[(128), float32] */, %conv3_1_weight: Tensor[(256, 128, 3, 3), float32] /* ty=Tensor[(256, 128, 3, 3), float32] */, %conv3_1_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv3_2_weight: Tensor[(256, 256, 3, 3), float32] /* ty=Tensor[(256, 256, 3, 3), float32] */, %conv3_2_bias: Tensor[(256), float32] /* ty=Tensor[(256), float32] */, %conv4_1_weight: Tensor[(512, 256, 3, 3), float32] /* ty=Tensor[(512, 256, 3, 3), float32] */, %conv4_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv4_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv4_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_1_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_1_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %conv5_2_weight: Tensor[(512, 512, 3, 3), float32] /* ty=Tensor[(512, 512, 3, 3), float32] */, %conv5_2_bias: Tensor[(512), float32] /* ty=Tensor[(512), float32] */, %fc6_weight: Tensor[(4096, 25088), float32] /* ty=Tensor[(4096, 25088), float32] */, %fc6_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc7_weight: Tensor[(4096, 4096), float32] /* ty=Tensor[(4096, 4096), float32] */, %fc7_bias: Tensor[(4096), float32] /* ty=Tensor[(4096), float32] */, %fc8_weight: Tensor[(1000, 4096), float32] /* ty=Tensor[(1000, 4096), float32] */, %fc8_bias: Tensor[(1000), float32] /* ty=Tensor[(1000), float32] */) -> Tensor[(1, 1000), float32] {\n",
            "  %0 = nn.conv2d(%data, %conv1_1_weight, padding=[1, 1, 1, 1], channels=64, kernel_size=[3, 3]);\n",
            "  %1 = on_device(%0, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %2 = nn.bias_add(%1, %conv1_1_bias);\n",
            "  %3 = nn.relu(%2);\n",
            "  %4 = nn.max_pool2d(%3, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
            "  %5 = nn.conv2d(%4, %conv2_1_weight, padding=[1, 1, 1, 1], channels=128, kernel_size=[3, 3]);\n",
            "  %6 = on_device(%5, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %7 = nn.bias_add(%6, %conv2_1_bias);\n",
            "  %8 = nn.relu(%7);\n",
            "  %9 = nn.max_pool2d(%8, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
            "  %10 = nn.conv2d(%9, %conv3_1_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
            "  %11 = on_device(%10, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %12 = nn.bias_add(%11, %conv3_1_bias);\n",
            "  %13 = nn.relu(%12);\n",
            "  %14 = nn.conv2d(%13, %conv3_2_weight, padding=[1, 1, 1, 1], channels=256, kernel_size=[3, 3]);\n",
            "  %15 = on_device(%14, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %16 = nn.bias_add(%15, %conv3_2_bias);\n",
            "  %17 = nn.relu(%16);\n",
            "  %18 = nn.max_pool2d(%17, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
            "  %19 = nn.conv2d(%18, %conv4_1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
            "  %20 = on_device(%19, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %21 = nn.bias_add(%20, %conv4_1_bias);\n",
            "  %22 = nn.relu(%21);\n",
            "  %23 = nn.conv2d(%22, %conv4_2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
            "  %24 = on_device(%23, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %25 = nn.bias_add(%24, %conv4_2_bias);\n",
            "  %26 = nn.relu(%25);\n",
            "  %27 = nn.max_pool2d(%26, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
            "  %28 = nn.conv2d(%27, %conv5_1_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
            "  %29 = on_device(%28, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %30 = nn.bias_add(%29, %conv5_1_bias);\n",
            "  %31 = nn.relu(%30);\n",
            "  %32 = nn.conv2d(%31, %conv5_2_weight, padding=[1, 1, 1, 1], channels=512, kernel_size=[3, 3]);\n",
            "  %33 = on_device(%32, virtual_device=VirtualDevice(device_type=2, virtual_device_id=0));\n",
            "  %34 = nn.bias_add(%33, %conv5_2_bias);\n",
            "  %35 = nn.relu(%34);\n",
            "  %36 = nn.max_pool2d(%35, pool_size=[2, 2], strides=[2, 2], padding=[0, 0, 0, 0]);\n",
            "  %37 = nn.batch_flatten(%36);\n",
            "  %38 = nn.dense(%37, %fc6_weight, units=4096);\n",
            "  %39 = nn.bias_add(%38, %fc6_bias, axis=-1);\n",
            "  %40 = nn.relu(%39);\n",
            "  %41 = nn.dropout(%40);\n",
            "  %42 = %41.0;\n",
            "  %43 = nn.dense(%42, %fc7_weight, units=4096);\n",
            "  %44 = nn.bias_add(%43, %fc7_bias, axis=-1);\n",
            "  %45 = nn.relu(%44);\n",
            "  %46 = nn.dropout(%45);\n",
            "  %47 = %46.0;\n",
            "  %48 = nn.dense(%47, %fc8_weight, units=1000);\n",
            "  %49 = nn.bias_add(%48, %fc8_bias, axis=-1);\n",
            "  nn.softmax(%49)\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the Raly IR to generate LLVM code and CUDA code\n",
        "with tvm.transform.PassContext(opt_level=3):\n",
        "  graph, lib, params = relay.build(mod, target={'cpu':'llvm','gpu':'cuda'}, params=params)\n",
        "\n",
        "# Create the runtime module for the generated LLVM code and CUDA code\n",
        "from tvm.contrib import graph_runtime\n",
        "ctx = [tvm.cpu(0), tvm.cuda(0)]\n",
        "run_mod = graph_runtime.create(graph, lib, ctx)\n",
        "\n",
        "# Run inference 10 times\n",
        "import time\n",
        "import numpy as np\n",
        "times = []\n",
        "for _ in range(10):\n",
        "  start = time.time()\n",
        "  run_mod.run()\n",
        "  times.append(time.time() - start)\n",
        "print(\"Median inference latency %.2f ms\" % (1000 * np.median(times)))"
      ],
      "metadata": {
        "id": "jiz_ReQUaJHB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0fc17696-e7e4-4a49-ad23-bc4c86a60a1e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-3b6fbc22ea94>:3: DeprecationWarning: legacy graph executor behavior of producing json / lib / params will be removed in the next release. Please see documents of tvm.contrib.graph_executor.GraphModule for the  new recommended usage.\n",
            "  graph, lib, params = relay.build(mod, target={'cpu':'llvm','gpu':'cuda'}, params=params)\n",
            "/usr/local/lib/python3.10/dist-packages/tvm/contrib/graph_runtime.py:25: UserWarning: This function has been moved to tvm.contrib.graph_executor and will be removed in the next TVM release\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Median inference latency 99.60 ms\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.18"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}